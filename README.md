# Attention Is All You Need - Transformer Implementation from Scratch

## Overview
This repository contains a from-scratch implementation of the Transformer model, based on the seminal paper ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762) by Vaswani et al. The model is built using Python and PyTorch, without relying on high-level libraries like Hugging Face's `transformers`.

## Features
- Implements the complete Transformer architecture
- Includes **multi-head self-attention**, **positional encodings**, and **feed-forward networks**
- Trains on a sample dataset for **machine translation (English â†’ French/German/etc.)**
- Provides visualization of attention weights
- Modular and well-documented code for easy understanding


## References
- [Attention Is All You Need (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)

## Contributing
Feel free to submit issues and pull requests to improve the implementation. Contributions are welcome!

## License
MIT License. See [LICENSE](LICENSE) for details.

